
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Evaluation Metrics &#8212; CAPRI 0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Recommendation Algorithms" href="recommendation-algorithms.html" />
    <link rel="prev" title="Quickstart guide" href="quickstart.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="evaluation-metrics">
<h1>Evaluation Metrics<a class="headerlink" href="#evaluation-metrics" title="Permalink to this headline">¶</a></h1>
<p>Many evaluation metrics are available for recommendation systems and each has its own pros and cons.
<code class="docutils literal notranslate"><span class="pre">CAPRI</span></code> supports the two types of evaluation metrics: <strong>Accuracy</strong> and <strong>Beyond Accuracy</strong>.
In this page, we will discuss these two evaluation metrics in detail.</p>
<section id="accuracy-metrics">
<h2>Accuracy Metrics<a class="headerlink" href="#accuracy-metrics" title="Permalink to this headline">¶</a></h2>
<section id="precision-k">
<h3>Precision&#64;K<a class="headerlink" href="#precision-k" title="Permalink to this headline">¶</a></h3>
<p>Precision at k is the fraction of relevant recommended items in the top-k set.
Assume that in a top-10 recommendation problem, my precision at 10 is 75%.
This means that 75% of the suggestions I offer are applicable to the user.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">precisionk</span><span class="p">(</span><span class="n">actual</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">recommended</span><span class="p">:</span> <span class="nb">list</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the number of relevant results among the top k recommended items</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    actual: list</span>
<span class="sd">        A list of ground truth items</span>
<span class="sd">        example: [X, Y, Z]</span>
<span class="sd">    recommended: list</span>
<span class="sd">        A list of ground truth items (all possible relevant items)</span>
<span class="sd">        example: [x, y, z]</span>

<span class="sd">    Returns</span>
<span class="sd">    ----------</span>
<span class="sd">        precision at k</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">relevantResults</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span> <span class="o">&amp;</span> <span class="nb">set</span><span class="p">(</span><span class="n">recommended</span><span class="p">)</span>
    <span class="k">assert</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="nb">len</span><span class="p">(</span>
        <span class="n">relevantResults</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;The number of relevant results is not true (currently: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">relevantResults</span><span class="p">)</span><span class="si">}</span><span class="s2">)&quot;</span>
    <span class="k">return</span> <span class="mf">1.0</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">relevantResults</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">recommended</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="recall-k">
<h3>Recall&#64;K<a class="headerlink" href="#recall-k" title="Permalink to this headline">¶</a></h3>
<p>The proportion of relevant things discovered in the top-k suggestions is known as recall at k.
Assume we computed recall at 10 and discovered it to be 55% in our top-10 recommendation system.
This suggests that the top-k results contain 55% of the entire number of relevant items.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">recallk</span><span class="p">(</span><span class="n">actual</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">recommended</span><span class="p">:</span> <span class="nb">list</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The number of relevant results among the top k recommended items divided by the total number of relevant items</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    actual: list</span>
<span class="sd">        A list of ground truth items (all possible relevant items)</span>
<span class="sd">        example: [X, Y, Z]</span>
<span class="sd">    recommended: list</span>
<span class="sd">        A list of items recommended by the system</span>
<span class="sd">        example: [x, y, z]</span>

<span class="sd">    Returns</span>
<span class="sd">    ----------</span>
<span class="sd">        recall at k</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">relevantResults</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span> <span class="o">&amp;</span> <span class="nb">set</span><span class="p">(</span><span class="n">recommended</span><span class="p">)</span>
    <span class="k">assert</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">relevantResults</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;The number of relevant results is not true (currently: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">relevantResults</span><span class="p">)</span><span class="si">}</span><span class="s2">)&quot;</span>
    <span class="k">return</span> <span class="mf">1.0</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">relevantResults</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="map-k">
<h3>Map&#64;K<a class="headerlink" href="#map-k" title="Permalink to this headline">¶</a></h3>
<p>MAP&#64;k stands for Mean Average Precision at Cut Off k and is most commonly employed in recommendation systems,
although it can also be utilized in other systems.
When you use MAP to evaluate a recommender algorithm, you’re treating the recommendation as a ranking problem.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">mapk</span><span class="p">(</span><span class="n">actual</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">predicted</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes mean Average Precision at k (mAPk) for a set of recommended items</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    actual: list</span>
<span class="sd">        A list of ground truth items (order doesn&#39;t matter)</span>
<span class="sd">        example: [X, Y, Z]</span>
<span class="sd">    predicted: list</span>
<span class="sd">        A list of predicted items, recommended by the system (order matters)</span>
<span class="sd">        example: [x, y, z]</span>
<span class="sd">    k: integer, optional (default to 10)</span>
<span class="sd">        The number of elements of predicted to consider in the calculation</span>

<span class="sd">    Returns</span>
<span class="sd">    ----------</span>
<span class="sd">    score:</span>
<span class="sd">        The mean Average Precision at k (mAPk)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">score</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">numberOfHits</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">predicted</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">actual</span> <span class="ow">and</span> <span class="n">p</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">predicted</span><span class="p">[:</span><span class="n">i</span><span class="p">]:</span>
            <span class="n">numberOfHits</span> <span class="o">+=</span> <span class="mf">1.0</span>
            <span class="n">score</span> <span class="o">+=</span> <span class="n">numberOfHits</span> <span class="o">/</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mf">1.0</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">actual</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">0.0</span>
    <span class="n">score</span> <span class="o">=</span> <span class="n">score</span> <span class="o">/</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">actual</span><span class="p">),</span> <span class="n">k</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">score</span>
</pre></div>
</div>
</section>
<section id="ndcg-k">
<h3>NDCG&#64;K<a class="headerlink" href="#ndcg-k" title="Permalink to this headline">¶</a></h3>
<p>The Discounted Cumulative Gain for k displayed recommendations sums up the importance of the items displayed for
the current user (cumulative), while penalizing relevant items in later slots (discounted).
In the Normalized Cumulative Gain for k given suggestions, this score is divided by the maximum possible value of DCG&#64;K for the current user.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">dcg</span><span class="p">(</span><span class="n">scores</span><span class="p">:</span> <span class="nb">list</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the Discounted Cumulative Gain (DCG) for a list of scores</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    scores: list</span>
<span class="sd">        A list of scores</span>

<span class="sd">    Returns</span>
<span class="sd">    ----------</span>
<span class="sd">    dcg: float</span>
<span class="sd">        The Discounted Cumulative Gain (DCG)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">divide</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">scores</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">ndcgk</span><span class="p">(</span><span class="n">actual</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">predicted</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">relevance</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">at</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculates the implicit version of Normalized Discounted Cumulative Gain (NDCG) for top k items in the ranked output</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    actual: list</span>
<span class="sd">        A list of ground truth items</span>
<span class="sd">        example: [X, Y, Z]</span>
<span class="sd">    predicted: list</span>
<span class="sd">        A list of predicted items, recommended by the system</span>
<span class="sd">        example: [x, y, z]</span>
<span class="sd">    relevance: list, optional (default to None)</span>
<span class="sd">        A list of relevance scores for the items in the ground truth</span>
<span class="sd">    at: any, optional (default to None)</span>
<span class="sd">        The number of items to consider in the calculation</span>

<span class="sd">    Returns</span>
<span class="sd">    ----------</span>
<span class="sd">    ndcg:</span>
<span class="sd">        Normalized DCG score</span>

<span class="sd">    Metric Defintion</span>
<span class="sd">    ----------</span>
<span class="sd">    Jarvelin, K., &amp; Kekalainen, J. (2002). Cumulated gain-based evaluation of IR techniques.</span>
<span class="sd">    ACM Transactions on Information Systems (TOIS), 20(4), 422-446.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Convert list to numpy array</span>
    <span class="n">actual</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">actual</span><span class="p">)),</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">predicted</span><span class="p">))</span>
    <span class="c1"># Check the relevance value</span>
    <span class="k">if</span> <span class="n">relevance</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">relevance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">actual</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">relevance</span><span class="p">)</span> <span class="o">==</span> <span class="n">actual</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="c1"># Creating a dictionary associating itemId to its relevance</span>
    <span class="n">item2rel</span> <span class="o">=</span> <span class="p">{</span><span class="n">it</span><span class="p">:</span> <span class="n">r</span> <span class="k">for</span> <span class="n">it</span><span class="p">,</span> <span class="n">r</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">actual</span><span class="p">,</span> <span class="n">relevance</span><span class="p">)}</span>
    <span class="c1"># Creates array of length &quot;at&quot; with the relevance associated to the item in that position</span>
    <span class="n">rankScores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="n">item2rel</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">it</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
                            <span class="k">for</span> <span class="n">it</span> <span class="ow">in</span> <span class="n">predicted</span><span class="p">[:</span><span class="n">at</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="c1"># IDCG has all relevances to 1, up to the number of items in the test set</span>
    <span class="n">idcg</span> <span class="o">=</span> <span class="n">dcg</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">relevance</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="c1"># Calculating rank-DCG, DCG uses the relevance of the recommended items</span>
    <span class="n">rdcg</span> <span class="o">=</span> <span class="n">dcg</span><span class="p">(</span><span class="n">rankScores</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">rdcg</span> <span class="o">==</span> <span class="mf">0.0</span><span class="p">:</span>
        <span class="k">return</span> <span class="mf">0.0</span>
    <span class="c1"># Return items</span>
    <span class="k">return</span> <span class="nb">round</span><span class="p">(</span><span class="n">rdcg</span> <span class="o">/</span> <span class="n">idcg</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="beyond-accuracy-metrics">
<h2>Beyond-accuracy Metrics<a class="headerlink" href="#beyond-accuracy-metrics" title="Permalink to this headline">¶</a></h2>
<p>Beyound accuracy metrics refer to evaluating recommender systems by coverage and serendipity.</p>
<section id="list-diversity">
<h3>List Diversity<a class="headerlink" href="#list-diversity" title="Permalink to this headline">¶</a></h3>
<p>List Diversity is one of the most common metrics used to evaluate recommender systems.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">listDiversity</span><span class="p">(</span><span class="n">predicted</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">itemsSimilarityMatrix</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the diversity for a list of recommended items for a user</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    predicted: list</span>
<span class="sd">        A list of predicted numeric/character vectors of retrieved documents for the corresponding element of actual</span>
<span class="sd">        example: [&#39;X&#39;, &#39;Y&#39;, &#39;Z&#39;]</span>

<span class="sd">    Returns</span>
<span class="sd">    ----------</span>
<span class="sd">        diversity</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">pairCount</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">similarity</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">pairs</span> <span class="o">=</span> <span class="n">itertools</span><span class="o">.</span><span class="n">combinations</span><span class="p">(</span><span class="n">predicted</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">:</span>
        <span class="n">itemID1</span> <span class="o">=</span> <span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">itemID2</span> <span class="o">=</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">similarity</span> <span class="o">+=</span> <span class="n">itemsSimilarityMatrix</span><span class="p">[</span><span class="n">itemID1</span><span class="p">,</span> <span class="n">itemID2</span><span class="p">]</span>
        <span class="n">pairCount</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">averageSimilarity</span> <span class="o">=</span> <span class="n">similarity</span> <span class="o">/</span> <span class="n">pairCount</span>
    <span class="n">diversity</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">averageSimilarity</span>
    <span class="k">return</span> <span class="n">diversity</span>
</pre></div>
</div>
</section>
<section id="novelty">
<h3>Novelty<a class="headerlink" href="#novelty" title="Permalink to this headline">¶</a></h3>
<p>Novelty is one of the most common metrics used to evaluate recommender systems.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">novelty</span><span class="p">(</span><span class="n">predicted</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">pop</span><span class="p">:</span> <span class="nb">dict</span><span class="p">,</span> <span class="n">u</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the novelty for a list of recommended items for a user</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    predicted: a list of recommedned items</span>
<span class="sd">        Ordered predictions</span>
<span class="sd">        example: [&#39;X&#39;, &#39;Y&#39;, &#39;Z&#39;]</span>
<span class="sd">    pop: dictionary</span>
<span class="sd">        A dictionary of all items alongside of its occurrences counter in the training data</span>
<span class="sd">        example: {1198: 893, 1270: 876, 593: 876, 2762: 867}</span>
<span class="sd">    u: integer</span>
<span class="sd">        The number of users in the training data</span>
<span class="sd">    k: integer</span>
<span class="sd">        The length of recommended lists per user</span>

<span class="sd">    Returns</span>
<span class="sd">    ----------</span>
<span class="sd">    novelty:</span>
<span class="sd">        The novelty of the recommendations in system level</span>

<span class="sd">    Metric Definition</span>
<span class="sd">    ----------</span>
<span class="sd">    Zhou, T., Kuscsik, Z., Liu, J. G., Medo, M., Wakeling, J. R., &amp; Zhang, Y. C. (2010).</span>
<span class="sd">    Solving the apparent diversity-accuracy dilemma of recommender systems.</span>
<span class="sd">    Proceedings of the National Academy of Sciences, 107(10), 4511-4515.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">selfInformation</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">predicted</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">pop</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">itemPopularity</span> <span class="o">=</span> <span class="n">pop</span><span class="p">[</span><span class="n">item</span><span class="p">]</span><span class="o">/</span><span class="n">u</span>
            <span class="n">itemNoveltyValue</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="n">itemPopularity</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">itemNoveltyValue</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">selfInformation</span> <span class="o">+=</span> <span class="n">itemNoveltyValue</span>
    <span class="n">noveltyScore</span> <span class="o">=</span> <span class="n">selfInformation</span><span class="o">/</span><span class="n">k</span>
    <span class="k">return</span> <span class="n">noveltyScore</span>
</pre></div>
</div>
</section>
<section id="catalog-coverage">
<h3>Catalog Coverage<a class="headerlink" href="#catalog-coverage" title="Permalink to this headline">¶</a></h3>
<p>Catalog Coverage is one of the most common metrics used to evaluate recommender systems.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">catalogCoverage</span><span class="p">(</span><span class="n">predicted</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">list</span><span class="p">],</span> <span class="n">catalog</span><span class="p">:</span> <span class="nb">set</span><span class="p">):</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Computes the catalog coverage for k lists of recommendations</span>
<span class="sd">Coverage is the percent of items in the training data the model is able to recommend on a test set</span>

<span class="sd">Parameters</span>
<span class="sd">----------</span>
<span class="sd">predicted: a list of lists</span>
<span class="sd">    Ordered predictions</span>
<span class="sd">    example: [[&#39;X&#39;, &#39;Y&#39;, &#39;Z&#39;], [&#39;X&#39;, &#39;Y&#39;, &#39;Z&#39;]]</span>
<span class="sd">catalog: list</span>
<span class="sd">    A list of all unique items in the training data</span>
<span class="sd">    example: [&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;X&#39;, &#39;Y&#39;, &#39;Z&#39;]</span>
<span class="sd">k: integer</span>
<span class="sd">    The number of observed recommendation lists</span>
<span class="sd">    which randomly choosed in our offline setup</span>

<span class="sd">Returns</span>
<span class="sd">----------</span>
<span class="sd">catalogCoverage:</span>
<span class="sd">    The catalog coverage of the recommendations as a percent</span>
<span class="sd">    rounded to 2 decimal places</span>

<span class="sd">Metric Definition</span>
<span class="sd">-----------------</span>
<span class="sd">Ge, M., Delgado-Battenfeld, C., &amp; Jannach, D. (2010, September).</span>
<span class="sd">Beyond accuracy: evaluating recommender systems by coverage and serendipity.</span>
<span class="sd">In Proceedings of the fourth ACM conference on Recommender systems (pp. 257-260). ACM.</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">predictedFlattened</span> <span class="o">=</span> <span class="p">[</span><span class="n">p</span> <span class="k">for</span> <span class="n">sublist</span> <span class="ow">in</span> <span class="n">predicted</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">sublist</span><span class="p">]</span>
<span class="n">LPredictions</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">predictedFlattened</span><span class="p">))</span>
<span class="n">catalogCoverage</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="n">LPredictions</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">catalog</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="k">return</span> <span class="n">catalogCoverage</span>
</pre></div>
</div>
</section>
<section id="personalization">
<h3>Personalization<a class="headerlink" href="#personalization" title="Permalink to this headline">¶</a></h3>
<p>Personalization is one of the most common metrics used to evaluate recommender systems.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">personalization</span><span class="p">(</span><span class="n">predicted</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">list</span><span class="p">]):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Personalization measures recommendation similarity across users.</span>
<span class="sd">    A high score indicates good personalization (user&#39;s lists of recommendations are different).</span>
<span class="sd">    A low score indicates poor personalization (user&#39;s lists of recommendations are very similar).</span>
<span class="sd">    A model is &quot;personalizing&quot; well if the set of recommendations for each user is different.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    predicted: a list of lists</span>
<span class="sd">        Ordered predictions</span>
<span class="sd">        example: [[&#39;X&#39;, &#39;Y&#39;, &#39;Z&#39;], [&#39;X&#39;, &#39;Y&#39;, &#39;Z&#39;]]</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">        The personalization score for all recommendations.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">makeRecMatrix</span><span class="p">(</span><span class="n">predicted</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">list</span><span class="p">]):</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">predicted</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">()</span><span class="o">.</span><span class="n">melt</span><span class="p">(</span>
            <span class="n">id_vars</span><span class="o">=</span><span class="s1">&#39;index&#39;</span><span class="p">,</span> <span class="n">value_name</span><span class="o">=</span><span class="s1">&#39;item&#39;</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;index&#39;</span><span class="p">,</span> <span class="s1">&#39;item&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">pivot</span><span class="p">(</span>
            <span class="n">index</span><span class="o">=</span><span class="s1">&#39;index&#39;</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="s1">&#39;item&#39;</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="s1">&#39;item&#39;</span><span class="p">)</span>
        <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">notna</span><span class="p">(</span><span class="n">df</span><span class="p">)</span><span class="o">*</span><span class="mi">1</span>
        <span class="n">recMatrix</span> <span class="o">=</span> <span class="n">sp</span><span class="o">.</span><span class="n">csr_matrix</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">recMatrix</span>

    <span class="c1"># Create matrix for recommendations</span>
    <span class="n">predicted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">predicted</span><span class="p">)</span>
    <span class="n">recMatrixSparse</span> <span class="o">=</span> <span class="n">makeRecMatrix</span><span class="p">(</span><span class="n">predicted</span><span class="p">)</span>
    <span class="c1"># Calculate similarity for every user&#39;s recommendation list</span>
    <span class="n">similarity</span> <span class="o">=</span> <span class="n">cosine_similarity</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">recMatrixSparse</span><span class="p">,</span> <span class="n">dense_output</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="c1"># Get indicies for upper right triangle w/o diagonal</span>
    <span class="n">upperRight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">triu_indices</span><span class="p">(</span><span class="n">similarity</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># Calculate average similarity</span>
    <span class="n">personalization</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">similarity</span><span class="p">[</span><span class="n">upperRight</span><span class="p">])</span>
    <span class="k">return</span> <span class="mi">1</span><span class="o">-</span><span class="n">personalization</span>
</pre></div>
</div>
</section>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="index.html">
              <img class="logo" src="_static/logo.svg" alt="Logo"/>
            </a></p>
<h1 class="logo"><a href="index.html">CAPRI</a></h1>



<p class="blurb">The CAPRI project aims to automate contextual POI recommendation algorithms.</p>




<p>
<iframe src="https://ghbtns.com/github-btn.html?user=CapriRecSys&repo=CAPRI&type=watch&count=true&size=large&v=2"
  allowtransparency="true" frameborder="0" scrolling="0" width="200px" height="35px"></iframe>
</p>





<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart guide</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Evaluation Metrics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#accuracy-metrics">Accuracy Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="#beyond-accuracy-metrics">Beyond-accuracy Metrics</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="recommendation-algorithms.html">Recommendation Algorithms</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="quickstart.html" title="previous chapter">Quickstart guide</a></li>
      <li>Next: <a href="recommendation-algorithms.html" title="next chapter">Recommendation Algorithms</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2022, Ali Tourani, Hossein A. Rahmani, Mohammadmehdi Naghiaei, Yashar Deldjoo.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.2.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/evaluation-metrics.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>
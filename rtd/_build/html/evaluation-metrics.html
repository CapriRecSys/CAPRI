
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Evaluation Metrics &#8212; CAPRI 0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Recommendation Algorithms" href="recommendation-algorithms.html" />
    <link rel="prev" title="Quickstart guide" href="quickstart.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="evaluation-metrics">
<h1>Evaluation Metrics<a class="headerlink" href="#evaluation-metrics" title="Permalink to this headline">¶</a></h1>
<p>Many evaluation metrics are available for recommendation systems and each has its own pros and cons.
<code class="docutils literal notranslate"><span class="pre">CAPRI</span></code> supports the two types of evaluation metrics: <strong>Accuracy</strong> and <strong>Beyond Accuracy</strong>.
In this page, we will discuss these two evaluation metrics in detail.</p>
<section id="accuracy-metrics">
<h2>Accuracy Metrics<a class="headerlink" href="#accuracy-metrics" title="Permalink to this headline">¶</a></h2>
<section id="precision-k">
<h3>Precision&#64;K<a class="headerlink" href="#precision-k" title="Permalink to this headline">¶</a></h3>
<p>Precision at k is the fraction of relevant recommended items in the top-k set.
Assume that in a top-10 recommendation problem, my precision at 10 is 75%.
This means that 75% of the suggestions I offer are applicable to the user.</p>
<dl>
<dt>def precisionk(actual: list, recommended: list):</dt><dd><p>“””
Computes the number of relevant results among the top k recommended items</p>
<dl class="simple">
<dt>actual: list</dt><dd><p>A list of ground truth items
example: [X, Y, Z]</p>
</dd>
<dt>recommended: list</dt><dd><p>A list of ground truth items (all possible relevant items)
example: [x, y, z]</p>
</dd>
</dl>
<blockquote>
<div><p>precision at k</p>
</div></blockquote>
<p>“””
relevantResults = set(actual) &amp; set(recommended)
assert 0 &lt;= len(</p>
<blockquote>
<div><p>relevantResults), f”The number of relevant results is not true (currently: {len(relevantResults)})”</p>
</div></blockquote>
<p>return 1.0 * len(relevantResults) / len(recommended)</p>
</dd>
</dl>
</section>
<section id="recall-k">
<h3>Recall&#64;K<a class="headerlink" href="#recall-k" title="Permalink to this headline">¶</a></h3>
<p>The proportion of relevant things discovered in the top-k suggestions is known as recall at k.
Assume we computed recall at 10 and discovered it to be 55% in our top-10 recommendation system.
This suggests that the top-k results contain 55% of the entire number of relevant items.</p>
<dl>
<dt>def recallk(actual: list, recommended: list):</dt><dd><p>“””
The number of relevant results among the top k recommended items divided by the total number of relevant items</p>
<dl class="simple">
<dt>actual: list</dt><dd><p>A list of ground truth items (all possible relevant items)
example: [X, Y, Z]</p>
</dd>
<dt>recommended: list</dt><dd><p>A list of items recommended by the system
example: [x, y, z]</p>
</dd>
</dl>
<blockquote>
<div><p>recall at k</p>
</div></blockquote>
<p>“””
relevantResults = set(actual) &amp; set(recommended)
assert 0 &lt;= len(</p>
<blockquote>
<div><p>relevantResults), f”The number of relevant results is not true (currently: {len(relevantResults)})”</p>
</div></blockquote>
<p>return 1.0 * len(relevantResults) / len(actual)</p>
</dd>
</dl>
</section>
<section id="map-k">
<h3>Map&#64;K<a class="headerlink" href="#map-k" title="Permalink to this headline">¶</a></h3>
<p>MAP&#64;k stands for Mean Average Precision at Cut Off k and is most commonly employed in recommendation systems,
although it can also be utilized in other systems.
When you use MAP to evaluate a recommender algorithm, you’re treating the recommendation as a ranking problem.</p>
<dl>
<dt>def mapk(actual: list, predicted: list, k: int = 10):</dt><dd><p>“””
Computes mean Average Precision at k (mAPk) for a set of recommended items</p>
<dl class="simple">
<dt>actual: list</dt><dd><p>A list of ground truth items (order doesn’t matter)
example: [X, Y, Z]</p>
</dd>
<dt>predicted: list</dt><dd><p>A list of predicted items, recommended by the system (order matters)
example: [x, y, z]</p>
</dd>
<dt>k: integer, optional (default to 10)</dt><dd><p>The number of elements of predicted to consider in the calculation</p>
</dd>
</dl>
<dl class="simple">
<dt>score:</dt><dd><p>The mean Average Precision at k (mAPk)</p>
</dd>
</dl>
<p>“””
score = 0.0
numberOfHits = 0.0
for i, p in enumerate(predicted):</p>
<blockquote>
<div><dl class="simple">
<dt>if p in actual and p not in predicted[:i]:</dt><dd><p>numberOfHits += 1.0
score += numberOfHits / (i+1.0)</p>
</dd>
</dl>
</div></blockquote>
<dl class="simple">
<dt>if not actual:</dt><dd><p>return 0.0</p>
</dd>
</dl>
<p>score = score / min(len(actual), k)
return score</p>
</dd>
</dl>
</section>
<section id="ndcg-k">
<h3>NDCG&#64;K<a class="headerlink" href="#ndcg-k" title="Permalink to this headline">¶</a></h3>
<p>The Discounted Cumulative Gain for k displayed recommendations sums up the importance of the items displayed for
the current user (cumulative), while penalizing relevant items in later slots (discounted).
In the Normalized Cumulative Gain for k given suggestions, this score is divided by the maximum possible value of DCG&#64;K for the current user.</p>
<dl>
<dt>def dcg(scores: list):</dt><dd><p>“””
Computes the Discounted Cumulative Gain (DCG) for a list of scores</p>
<dl class="simple">
<dt>scores: list</dt><dd><p>A list of scores</p>
</dd>
</dl>
<dl class="simple">
<dt>dcg: float</dt><dd><p>The Discounted Cumulative Gain (DCG)</p>
</dd>
</dl>
<p>“””
return np.sum(np.divide(np.power(2, scores) - 1, np.log(np.arange(scores.shape[0], dtype=np.float32) + 2)),</p>
<blockquote>
<div><p>dtype=np.float32)</p>
</div></blockquote>
</dd>
<dt>def ndcgk(actual: list, predicted: list, relevance=None, at=None):</dt><dd><p>“””
Calculates the implicit version of Normalized Discounted Cumulative Gain (NDCG) for top k items in the ranked output</p>
<dl class="simple">
<dt>actual: list</dt><dd><p>A list of ground truth items
example: [X, Y, Z]</p>
</dd>
<dt>predicted: list</dt><dd><p>A list of predicted items, recommended by the system
example: [x, y, z]</p>
</dd>
<dt>relevance: list, optional (default to None)</dt><dd><p>A list of relevance scores for the items in the ground truth</p>
</dd>
<dt>at: any, optional (default to None)</dt><dd><p>The number of items to consider in the calculation</p>
</dd>
</dl>
<dl class="simple">
<dt>ndcg:</dt><dd><p>Normalized DCG score</p>
</dd>
</dl>
<p>Jarvelin, K., &amp; Kekalainen, J. (2002). Cumulated gain-based evaluation of IR techniques.
ACM Transactions on Information Systems (TOIS), 20(4), 422-446.
“””
# Convert list to numpy array
actual, predicted = np.asarray(list(actual)), np.asarray(list(predicted))
# Check the relevance value
if relevance is None:</p>
<blockquote>
<div><p>relevance = np.ones_like(actual)</p>
</div></blockquote>
<p>assert len(relevance) == actual.shape[0]
# Creating a dictionary associating itemId to its relevance
item2rel = {it: r for it, r in zip(actual, relevance)}
# Creates array of length “at” with the relevance associated to the item in that position
rankScores = np.asarray([item2rel.get(it, 0.0)</p>
<blockquote>
<div><p>for it in predicted[:at]], dtype=np.float32)</p>
</div></blockquote>
<p># IDCG has all relevances to 1, up to the number of items in the test set
idcg = dcg(np.sort(relevance)[::-1])
# Calculating rank-DCG, DCG uses the relevance of the recommended items
rdcg = dcg(rankScores)
if rdcg == 0.0:</p>
<blockquote>
<div><p>return 0.0</p>
</div></blockquote>
<p># Return items
return round(rdcg / idcg, 4)</p>
</dd>
</dl>
</section>
</section>
<section id="beyond-accuracy-metrics">
<h2>Beyond-accuracy Metrics<a class="headerlink" href="#beyond-accuracy-metrics" title="Permalink to this headline">¶</a></h2>
<p>Beyound accuracy metrics refer to evaluating recommender systems by coverage and serendipity.</p>
<section id="list-diversity">
<h3>List Diversity<a class="headerlink" href="#list-diversity" title="Permalink to this headline">¶</a></h3>
<p>List Diversity is one of the most common metrics used to evaluate recommender systems.</p>
<dl>
<dt>def listDiversity(predicted: list, itemsSimilarityMatrix):</dt><dd><p>“””
Computes the diversity for a list of recommended items for a user</p>
<dl class="simple">
<dt>predicted: list</dt><dd><p>A list of predicted numeric/character vectors of retrieved documents for the corresponding element of actual
example: [‘X’, ‘Y’, ‘Z’]</p>
</dd>
</dl>
<blockquote>
<div><p>diversity</p>
</div></blockquote>
<p>“””
pairCount = 0
similarity = 0
pairs = itertools.combinations(predicted, 2)
for pair in pairs:</p>
<blockquote>
<div><p>itemID1 = pair[0]
itemID2 = pair[1]
similarity += itemsSimilarityMatrix[itemID1, itemID2]
pairCount += 1</p>
</div></blockquote>
<p>averageSimilarity = similarity / pairCount
diversity = 1 - averageSimilarity
return diversity</p>
</dd>
</dl>
</section>
<section id="novelty">
<h3>Novelty<a class="headerlink" href="#novelty" title="Permalink to this headline">¶</a></h3>
<p>Novelty is one of the most common metrics used to evaluate recommender systems.</p>
<dl>
<dt>def novelty(predicted: list, pop: dict, u: int, k: int):</dt><dd><p>“””
Computes the novelty for a list of recommended items for a user</p>
<dl class="simple">
<dt>predicted: a list of recommedned items</dt><dd><p>Ordered predictions
example: [‘X’, ‘Y’, ‘Z’]</p>
</dd>
<dt>pop: dictionary</dt><dd><p>A dictionary of all items alongside of its occurrences counter in the training data
example: {1198: 893, 1270: 876, 593: 876, 2762: 867}</p>
</dd>
<dt>u: integer</dt><dd><p>The number of users in the training data</p>
</dd>
<dt>k: integer</dt><dd><p>The length of recommended lists per user</p>
</dd>
</dl>
<dl class="simple">
<dt>novelty:</dt><dd><p>The novelty of the recommendations in system level</p>
</dd>
</dl>
<p>Zhou, T., Kuscsik, Z., Liu, J. G., Medo, M., Wakeling, J. R., &amp; Zhang, Y. C. (2010).
Solving the apparent diversity-accuracy dilemma of recommender systems.
Proceedings of the National Academy of Sciences, 107(10), 4511-4515.
“””
selfInformation = 0
for item in predicted:</p>
<blockquote>
<div><dl class="simple">
<dt>if item in pop.keys():</dt><dd><p>itemPopularity = pop[item]/u
itemNoveltyValue = np.sum(-np.log2(itemPopularity))</p>
</dd>
<dt>else:</dt><dd><p>itemNoveltyValue = 0</p>
</dd>
</dl>
<p>selfInformation += itemNoveltyValue</p>
</div></blockquote>
<p>noveltyScore = selfInformation/k
return noveltyScore</p>
</dd>
</dl>
</section>
<section id="catalog-coverage">
<h3>Catalog Coverage<a class="headerlink" href="#catalog-coverage" title="Permalink to this headline">¶</a></h3>
<p>Catalog Coverage is one of the most common metrics used to evaluate recommender systems.</p>
<dl>
<dt>def catalogCoverage(predicted: List[list], catalog: set):</dt><dd><p>“””
Computes the catalog coverage for k lists of recommendations
Coverage is the percent of items in the training data the model is able to recommend on a test set</p>
<dl class="simple">
<dt>predicted: a list of lists</dt><dd><p>Ordered predictions
example: [[‘X’, ‘Y’, ‘Z’], [‘X’, ‘Y’, ‘Z’]]</p>
</dd>
<dt>catalog: list</dt><dd><p>A list of all unique items in the training data
example: [‘A’, ‘B’, ‘C’, ‘X’, ‘Y’, ‘Z’]</p>
</dd>
<dt>k: integer</dt><dd><p>The number of observed recommendation lists
which randomly choosed in our offline setup</p>
</dd>
</dl>
<dl class="simple">
<dt>catalogCoverage:</dt><dd><p>The catalog coverage of the recommendations as a percent
rounded to 2 decimal places</p>
</dd>
</dl>
<p>Ge, M., Delgado-Battenfeld, C., &amp; Jannach, D. (2010, September).
Beyond accuracy: evaluating recommender systems by coverage and serendipity.
In Proceedings of the fourth ACM conference on Recommender systems (pp. 257-260). ACM.
“””
predictedFlattened = [p for sublist in predicted for p in sublist]
LPredictions = len(set(predictedFlattened))
catalogCoverage = round(LPredictions / (len(catalog) * 1.0) * 100, 2)
return catalogCoverage</p>
</dd>
</dl>
</section>
<section id="personalization">
<h3>Personalization<a class="headerlink" href="#personalization" title="Permalink to this headline">¶</a></h3>
<p>Personalization is one of the most common metrics used to evaluate recommender systems.</p>
<dl>
<dt>def personalization(predicted: List[list]):</dt><dd><p>“””
Personalization measures recommendation similarity across users.
A high score indicates good personalization (user’s lists of recommendations are different).
A low score indicates poor personalization (user’s lists of recommendations are very similar).
A model is “personalizing” well if the set of recommendations for each user is different.</p>
<dl class="simple">
<dt>predicted: a list of lists</dt><dd><p>Ordered predictions
example: [[‘X’, ‘Y’, ‘Z’], [‘X’, ‘Y’, ‘Z’]]</p>
</dd>
</dl>
<blockquote>
<div><p>The personalization score for all recommendations.</p>
</div></blockquote>
<p>“””</p>
<dl>
<dt>def makeRecMatrix(predicted: List[list]):</dt><dd><dl class="simple">
<dt>df = pd.DataFrame(data=predicted).reset_index().melt(</dt><dd><p>id_vars=’index’, value_name=’item’,</p>
</dd>
</dl>
<p>)
df = df[[‘index’, ‘item’]].pivot(</p>
<blockquote>
<div><p>index=’index’, columns=’item’, values=’item’)</p>
</div></blockquote>
<p>df = pd.notna(df)*1
recMatrix = sp.csr_matrix(df.values)
return recMatrix</p>
</dd>
</dl>
<p># Create matrix for recommendations
predicted = np.array(predicted)
recMatrixSparse = makeRecMatrix(predicted)
# Calculate similarity for every user’s recommendation list
similarity = cosine_similarity(X=recMatrixSparse, dense_output=False)
# Get indicies for upper right triangle w/o diagonal
upperRight = np.triu_indices(similarity.shape[0], k=1)
# Calculate average similarity
personalization = np.mean(similarity[upperRight])
return 1-personalization</p>
</dd>
</dl>
</section>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="index.html">
              <img class="logo" src="_static/logo.svg" alt="Logo"/>
            </a></p>
<h1 class="logo"><a href="index.html">CAPRI</a></h1>



<p class="blurb">The CAPRI project aims to automate contextual POI recommendation algorithms.</p>




<p>
<iframe src="https://ghbtns.com/github-btn.html?user=CapriRecSys&repo=CAPRI&type=watch&count=true&size=large&v=2"
  allowtransparency="true" frameborder="0" scrolling="0" width="200px" height="35px"></iframe>
</p>





<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="quickstart.html">Quickstart guide</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Evaluation Metrics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#accuracy-metrics">Accuracy Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="#beyond-accuracy-metrics">Beyond-accuracy Metrics</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="recommendation-algorithms.html">Recommendation Algorithms</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="quickstart.html" title="previous chapter">Quickstart guide</a></li>
      <li>Next: <a href="recommendation-algorithms.html" title="next chapter">Recommendation Algorithms</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2022, Ali Tourani, Hossein A. Rahmani, Mohammadmehdi Naghiaei, Yashar Deldjoo.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.5.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/evaluation-metrics.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>